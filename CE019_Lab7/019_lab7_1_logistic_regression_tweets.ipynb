{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"019_lab7_1_logistic_regression_tweets.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPiIHrav1g44fnBolTKf1Xq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"gUQpNMPqK46x"},"source":["import nltk\n","from nltk.corpus import twitter_samples \n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-p5drrIMb1R"},"source":["nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNkDgVvOMgV1"},"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NWicwIqMksY"},"source":["#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","\n","\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","            \n","            # 1 remove stopwords\n","            # 2 remove punctuation\n","            if (word not in stopwords_english and word not in string.punctuation):\n","                # 3 stemming word\n","                stem_word = stemmer.stem(word)\n","                # 4 Add it to tweets_clean\n","                tweets_clean.append(stem_word)\n","\n","    return tweets_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V90bhXK-Mrsr"},"source":["#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n","  # a positive label '1'         or \n","  # a negative label '0', \n","\n","#then builds the freqs dictionary, where each key is a (word,label) tuple, \n","\n","#and the value is the count of its frequency within the corpus of tweets.\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, y)\n","            \n","            #Update the count of pair if present, set it to 1 otherwise\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1\n","            \n","    return freqs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3_0SZn5rMybN"},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","print(len(all_positive_tweets))\n","print(len(all_negative_tweets))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DsxcPFbQNUGP"},"source":["# split the data into two pieces, one for training and one for testing\n","\n","train_len = int((len(all_positive_tweets) * 0.8))\n","\n","test_pos = all_positive_tweets[train_len:]\n","train_pos = all_positive_tweets[:train_len]\n","test_neg = all_negative_tweets[train_len:]\n","train_neg = all_negative_tweets[:train_len]\n","\n","train_x = train_pos + train_neg \n","test_x = test_pos + test_neg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yfo_Gk3ZNYvL"},"source":["# combine positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wL_48SbLNh0-"},"source":["# create frequency dictionary\n","freqs = build_freqs(train_x, train_y)\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLGtc3kaNmht"},"source":["# Example\n","print('This is an example of a positive tweet: \\n', train_x[0])\n","print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dAFOqqCNuOs"},"source":["def sigmoid(z): \n","       \n","    # calculate the sigmoid of z\n","    h = 1/(1+np.exp(-z))\n","    \n","    return h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGGCXHdVOlLc"},"source":["def gradientDescent(x, y, theta, alpha, num_iters):\n","  \n","    # get 'm', the number of rows in matrix x\n","    m = len(x)\n","    \n","    for i in range(0, num_iters):\n","        \n","        # get z, the dot product of x and theta\n","        z = np.dot(x,theta)\n","        \n","        # get the sigmoid of z\n","        h = sigmoid(z)\n","        \n","        # calculate the cost function\n","        J = (-1/m)*(y.T @ np.log(h) + (1-y).T @ np.log(1-h))\n","\n","        # update the weights theta\n","        theta = theta - (alpha/m) * np.dot(x.transpose(),(h-y))\n","        \n","    J = float(J)\n","    return J, theta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vttHPdNuOmkY"},"source":["def extract_features(tweet, freqs):\n","    '''\n","    Input: \n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output: \n","        x: a feature vector of dimension (1,3)\n","    '''\n","    # tokenizes, stems, and removes stopwords\n","    word_l = process_tweet(tweet)\n","    \n","    # 3 elements in the form of a 1 x 3 vector\n","    x = np.zeros((1, 3)) \n","    \n","    #bias term is set to 1\n","    x[0,0] = 1 \n","        \n","    # loop through each word in the list of words\n","    for word in word_l:\n","        \n","        # increment the word count for the positive label 1\n","        x[0,1] += freqs.get((word, 1.0),0)\n","        \n","        # increment the word count for the negative label 0\n","        x[0,2] += freqs.get((word, 0.0),0)\n","        \n","    \n","    assert(x.shape == (1, 3))\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QW-ux4D2OrN5"},"source":["# Check the function\n","\n","# test 1\n","# test on training data\n","tmp1 = extract_features(train_x[0], freqs)\n","print(tmp1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4MhddWuOvPg"},"source":["# test 2:\n","# check for when the words are not in the freqs dictionary\n","tmp2 = extract_features('Neel Boda', freqs)\n","print(tmp2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeVEMKPZPAJ7"},"source":["# collect the features 'x' and stack them into a matrix 'X'\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","    X[i, :]= extract_features(train_x[i], freqs)\n","\n","# training labels corresponding to X\n","Y = train_y\n","\n","# Apply gradient descent\n","J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n","print(f\"The cost after training is {J:.8f}.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"873sJplcPGj_"},"source":["def predict_tweet(tweet, freqs, theta):\n","    '''\n","    Input: \n","        tweet: a string\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","        theta: (3,1) vector of weights\n","    Output: \n","        y_pred: the probability of a tweet being positive or negative\n","    '''\n","    \n","    # extract the features of the tweet and store it into x\n","    x = extract_features(tweet,freqs)\n","    \n","    # make the prediction using x and theta\n","    y_pred = sigmoid(np.dot(x,theta))\n","    \n","    \n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VF2WtUZ1PMWb"},"source":["# Run this cell to test your function\n","for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n","    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlgnr1TLRFlJ"},"source":["def test_logistic_regression(test_x, test_y, freqs, theta):\n","    \"\"\"\n","    Input: \n","        test_x: a list of tweets\n","        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n","        freqs: a dictionary with the frequency of each pair (or tuple)\n","        theta: weight vector of dimension (3, 1)\n","    Output: \n","        accuracy: (# of tweets classified correctly) / (total # of tweets)\n","    \"\"\"\n","    \n","    \n","    # the list for storing predictions\n","    y_hat = []\n","    \n","    for tweet in test_x:\n","        # get the label prediction for the tweet\n","        y_pred = predict_tweet(tweet, freqs, theta)\n","        \n","        if y_pred > 0.5:\n","            # append 1.0 to the list\n","            y_hat.append(1)\n","        else:\n","            # append 0 to the list\n","            y_hat.append(0)\n","\n","    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n","    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n","    count=0\n","    y_hat=np.array(y_hat)\n","    m=len(test_y)\n","    #print(m)\n","    \n","    test_y=np.reshape(test_y,m)\n","    #print(y_hat.shape)\n","    #print(test_y.shape)\n","    \n","    accuracy = ((test_y == y_hat).sum())/m\n","    \n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DqNhrbCRLJf"},"source":["tmp_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\n","print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"],"execution_count":null,"outputs":[]}]}