{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"019_lab6_2_logistic_regression_tensorflow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMJUHpDK8HEQztcMKZnwEi6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"yQKdEQwktbkl"},"source":["from __future__ import absolute_import, division, print_function\n","import tensorflow as tf\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v74sD7OJHWb5"},"source":["from tensorflow.keras.datasets import mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Convert to float32.\n","x_train = np.array(x_train, np.float32)\n","x_test = np.array(x_test, np.float32)\n","\n","# Flatten images to 1-D vector of 784 features (28*28).\n","num_features=784\n","x_train = x_train.reshape(-1,num_features)\n","x_test = x_test.reshape(-1,num_features)\n","\n","# Normalize images value from [0, 255] to [0, 1].\n","x_train = x_train / 255\n","x_test = x_test /255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4NP-3GcHhRy"},"source":["# MNIST dataset parameters.\n","\n","num_classes = 10 # 0 to 9 digits\n","\n","num_features = 784 # 28*28\n","\n","# Training parameters.\n","\n","learning_rate = 0.01\n","\n","training_steps = 1000\n","\n","batch_size = 256\n","\n","display_step = 50"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozVL-laOJuOS"},"source":["# Use tf.data API to shuffle and batch data.\n","num_batches = int(x_train.shape[0]/batch_size)\n","\n","train_data=tf.data.Dataset.from_tensor_slices((x_train,y_train))\n","\n","train_data=train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7r3UxKoNJzWa"},"source":["# Weight of shape [784, 10], the 28*28 image features, and a total number of classes.\n","\n","W = tf.Variable(np.random.randn(784,10).astype(np.float32))\n","# X = tf.placeholder(tf.float32,[None,784])\n","# Y = tf.placeholder(tf.float32, [None,10])\n","\n","# Bias of shape [10], the total number of classes.\n","B = tf.Variable(np.random.randn(10).astype(np.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4jl8VF2J4Mf"},"source":["# Logistic regression (Wx + b).\n","\n","def logistic_regression(x):\n","\n","    # Apply softmax to normalize the logits to a probability distribution.\n","    return tf.nn.softmax(tf.add(tf.matmul(x,W),B))\n","    \n","\n","# Cross-Entropy loss function.\n","\n","def cross_entropy(y_pred, y_true):\n","\n","    # Encode label to a one hot vector.\n","    y_true = tf.one_hot(y_true, depth=num_classes)\n","    \n","    # Clip prediction values to avoid log(0) error.\n","    y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n","\n","    # Compute cross-entropy.\n","    ce_loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred)))\n","    return ce_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VY65xg9HJ8ZI"},"source":["# Accuracy metric.\n","\n","def accuracy(y_pred, y_true):\n","\n","  # Predicted class is the index of the highest score in prediction vector (i.e. argmax).\n","  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","\n","  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzjylH5oKBJE"},"source":["optimizer = tf.optimizers.SGD(learning_rate)\n","\n","def run_optimization(x, y):\n","\n","# Wrap computation inside a GradientTape for automatic differentiation.\n","\n","    with tf.GradientTape() as g:\n","\n","        pred = logistic_regression(x)\n","\n","        loss = cross_entropy(pred, y)\n","\n","    # Compute gradients.\n","\n","    gradients = g.gradient(loss, [W, B])\n","\n","\n","    # Stochastic gradient descent optimizer.\n","\n","    # Update W and b following gradients.\n","    optimizer.apply_gradients(zip(gradients, [W, B]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SL0EwaQlKFHD"},"source":["# Run training for the given number of steps.\n","\n","for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","\n","    # Run the optimization to update W and b values.\n","\n","    run_optimization(batch_x, batch_y)\n","\n","    \n","\n","    if step % display_step == 0:\n","\n","        #Obtain Predictions\n","        pred = logistic_regression(batch_x)\n","        #Ccompute loss\n","        loss = cross_entropy(pred, batch_y)\n","        #Compute Accuracy\n","        acc = accuracy(pred, batch_y)\n","        #print accuracy\n","        print(f\"step: {step}, loss: {loss}, accuracy: {acc}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjRXxMRGKLFN"},"source":["# Test model on validation set.\n","pred = logistic_regression(x_test)\n","accu = accuracy(pred, y_test)\n","print(f\"Test Accuracy: {accu}\")"],"execution_count":null,"outputs":[]}]}